{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ddaeb38",
   "metadata": {},
   "source": [
    "<figure><center>\n",
    "<img   src=\"../../Figuras/Fig_logo_UMA_scbi.png\" align=center  width=\"2000px\"/>\n",
    "</center></figure>\n",
    "\n",
    "$ \\newcommand{\\bra}[1]{\\langle #1|} $\n",
    "$ \\newcommand{\\ket}[1]{|#1\\rangle} $\n",
    "$ \\newcommand{\\braket}[2]{\\langle #1|#2\\rangle} $\n",
    "$ \\newcommand{\\i}{{\\color{blue} i}} $ \n",
    "$ \\newcommand{\\Hil}{{\\cal H}} $\n",
    "$ \\newcommand{\\cg}[1]{{\\rm C}#1} $\n",
    "$ \\newcommand{\\lp}{\\left(} $\n",
    "$ \\newcommand{\\rp}{\\right)} $\n",
    "$ \\newcommand{\\lc}{\\left[} $\n",
    "$ \\newcommand{\\rc}{\\right]} $\n",
    "$ \\newcommand{\\lch}{\\left\\{} $\n",
    "$ \\newcommand{\\rch}{\\right\\}} $\n",
    "$ \\newcommand{\\Lp}{\\Bigl(} $\n",
    "$ \\newcommand{\\Rp}{\\Bigr)} $\n",
    "$ \\newcommand{\\Lc}{\\Bigl[} $\n",
    "$ \\newcommand{\\Rc}{\\Bigr]} $\n",
    "$ \\newcommand{\\Lch}{\\Bigl\\{} $\n",
    "$ \\newcommand{\\Rch}{\\Bigr\\}} $\n",
    "$ \\newcommand{\\rqa}{\\quad \\Rightarrow \\quad} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "\\section{Probabilidades}\n",
    "\n",
    "La Mecánica Cuántica es \\textbf{intrinsecamente probabilística}. Por ello, es importate repasar algunos conceptos estadístico.\n",
    "\n",
    "\\subsection{Variables aleatorias}\n",
    "\n",
    "\\SubsubiIt{Variable aleatoria}\n",
    "\n",
    "\\begin{mybox_gray2}{}\n",
    "Denotamos con $(X,p(X))$ una  \\textbf{variable aleatoria} donde\n",
    "\\begin{itemize}\n",
    "\\item $X$ es el \\textbf{espacio muestral} de valores $\\{x_1, x_2,....,x_n\\}$ que pueden aparecer en una \\textit{consulta} a la variable aleatoria\n",
    "\n",
    "\\item $p(X)$ es la \\textbf{distribución de probabilidad}\n",
    "\\end{itemize}\n",
    "\\end{mybox_gray2}\n",
    "\n",
    "\\SubsubiIt{Distribución de probabilidad}\n",
    "\n",
    "\\begin{mybox_gray2}{}\n",
    "Una \\textbf{distribución de probabilidad} es una función real $x\\to p(x)$ que debe  verificar las dos condiciones siguientes\n",
    "\\begin{equation}\n",
    "p(x) \\in [0,1]~~~~~~~,~~~~~~~~\\sum_{x\\in X }p(x) = 1 \n",
    "\\end{equation}\n",
    "Es decir, la suma de probabilidades de todos los sucesos posibles debe ser la unidad.\n",
    "\\end{mybox_gray2}\n",
    "\n",
    "\\SubsubiIt{Media}\n",
    "\n",
    "\\begin{mybox_gray2}{}\n",
    "La \\textbf{media} de una variable aleatoria  viene dada por la expresión \n",
    "\\begin{equation}\n",
    "\\overline X  = \\sum_i x_i p(x_i)\n",
    "\\end{equation}\n",
    "\\end{mybox_gray2}	\n",
    "\n",
    "\\SubsubiIt{Varianza y desviación estándar}\n",
    "\n",
    "\\begin{mybox_gray2}{}\n",
    "La \\textbf{varianza}, $\\sigma_X^2$, es la  \\textit{media de la desviación cuadrática} $\\overline{(x_i - \\overline{X} )^2}$ \n",
    "\\begin{equation}\n",
    "\\sigma^2_X = \\sum_j (x_j-\\overline{X})^2 p(x_j) = \\overline{X^2} - \\overline{X}^2\n",
    "\\end{equation}\n",
    "La cantidad $\\sigma_X$ se denomina  \\textbf{desviación estándar}\n",
    "\\begin{equation}\n",
    "\\sigma_X = \\sqrt{\\overline{X^2} - \\overline{X}^2}\n",
    "\\end{equation}\n",
    "\\end{mybox_gray2}			\n",
    "\n",
    "\\subsection{La conexión estadística}\n",
    "\n",
    "Nuestro conocimiento del mundo se basa en la realización de \\textbf{experimentos}, el resultado de los cuales es (empíricamente) \\textbf{aleatorio}. Podemos pensar en el hecho de medir un sistema como la consulta de una variable aleatoria $(X,p(X))$ donde la distribución de probabilidad incorpora todo nuestro conocimiento acerca del sistema.\n",
    "\n",
    "\\SubsubiIt{Frecuencias e Histogramas}\n",
    "\n",
    "Cualquier consulta o medida da lugar a una \\textit{muestra} finita de valores $A_N = (a_1,a_2,...,a_N)$. Cada uno de estos resultados puede tomar cualquier valor $x_j$ del espacio muestral, es decir, $a_i\\in \\{x_1,...,x_n\\}$. A su vez, medidas diferentes ($a_i$ diferentes) pueden tomar valores iguales $x_j$, con números de aparición $n(x_i)$ tales que  $n(x_1) +  \\ldots + n(x_p) = N$. \n",
    "\n",
    "Estos datos se pueden agrupar en intervalos o \\textit{bins} que eliminen cierta precisión numérica. \n",
    "Por ejemplo, si truncamos nuestra precisión a las décimas de unidad,  $13.10$ y $13.19$ pertenecerán al mismo \\textit{bin}. \n",
    "\n",
    "Un \\textbf{histograma} es un diagrama en el que, por cada \\textit{bin}, hay una columna, cuya altura representa el número de sucesos que pertenecen a dicho \\textit{bin}. Podemos ver uno en la Fig. \\ref{Fig_formalismo_histograma}. \n",
    "\n",
    "\\begin{figure}[t]\n",
    "\\centering \n",
    "\\includegraphics[width=0.4\\linewidth]{Figuras/Fig_formalismo_histograma.png}\n",
    "\\caption{Ejemplo de histograma.}\n",
    "\\label{Fig_formalismo_histograma}\n",
    "\\end{figure}\n",
    "\n",
    "\\SubsubiIt{Ley de los grandes números}\n",
    "\n",
    "La conexión entre estadística y teoría de la probabilidad se da mediante la \\textbf{ley de los grandes números}. Lo que nos dice esta ley es que cuando el número de muestras tiende a infinito, $N\\to \\infty$, las fracciones relativas tienden a un número fijo que hereda, de toda la dinámica del sistema, ciertas propiedades que no se desvanecen. Este número es la probabilidad, es decir, \n",
    "\\begin{equation}\n",
    "f_N(x_i) = \\frac{n(x_i)}{N}~~~\\stackrel{N\\to\\infty}{\\longrightarrow}~~~{p(x_i)}\n",
    "\\end{equation}\n",
    "Un punto importante aquí es darnos cuenta de que experimentalmente sólo tenemos acceso a las frecuencias relativas $f_N(x_i)$ para un $N$ grande aunque \\textbf{finito}.\n",
    "\n",
    "Igualmente, nuestro conocimiento de la  media $\\overline X$  y la varianza $\\sigma_X^2$ siempre es aproximado, y se realiza a través de las medias y varianzas muestrales\n",
    "\\begin{eqnarray}\n",
    "\\overline{A}_N = \\sum_i x_i f_N(x_i)~~~&\\stackrel{N\\to\\infty}{\\longrightarrow}&~~~ \\overline{X}\\\\\n",
    "\\sigma_{A_N}^2 = \\sum_{i} (x_i - \\overline{A}_N)^2 f_N(x_i) ~~~&\\stackrel{N\\to\\infty}{\\longrightarrow}&~~~ \\sigma_X^2\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{mybox_blue}{Nota}\n",
    "Este es el enfoque \\textbf{frecuentista}.\n",
    "\\end{mybox_blue}\n",
    "\n",
    "\\SubsubiIt{La distribución de Bernouilli}\n",
    "\\begin{mybox_gray2}{}\n",
    "Una \\textbf{variable aleatoria de Bernouilli} $X=(x,p(x))$ tiene dos posibles resultados\n",
    "\\begin{itemize}\n",
    "\\item \\textbf{Exito}:$\\to x=1$ con probabilidad $p(1) = p$\n",
    "\n",
    "\\item \\textbf{Fracaso}: $\\to x=0$ con probabilidad $p(0) = 1-p$\n",
    "\\end{itemize}\n",
    "\\end{mybox_gray2}\n",
    "\n",
    "Podemos calcular fácilmente \n",
    "\\begin{align}\n",
    "\\overline X & = \\sum_i x_i p_i = 1 \\cdot p +0 \\cdot (1-p) = p \\\\\n",
    "\\sigma^2 & = \\sum_i (x-\\overline X)^2p_i = (1-p)^2p +(0-p) = p(1-p)\n",
    "\\end{align}\n",
    "\n",
    "\\SubsubiIt{La distribución Binomial}\n",
    "\n",
    "\\begin{mybox_gray2}{}\n",
    "La \\textbf{variable aleatoria binomia}l $X = (x,p(x))$ se define como\n",
    "\\begin{equation}\n",
    "x = \\text{número de éxitos obtenidos en } n \\text{ pruebas de Bernouilli sucesivas}\n",
    "\\end{equation}\n",
    "Claramente $x \\in (0,1,2,...n)$.\n",
    "\n",
    "Ahora es muy sencillo obtener la \\textbf{probabilidad} de un suceso con $x$ éxitos\n",
    "\\begin{equation}\n",
    "p(x) = \\begin{pmatrix}n\\\\ x\\end{pmatrix} p^x (1-p)^{n-x}\n",
    "\\end{equation}\n",
    "donde el primer factor tiene en cuenta las posibles ordenaciones en que aparecen $x$ éxitos en $n$ intentos.\n",
    "\\end{mybox_gray2}\n",
    "\n",
    "Podemos ver esta distribución en la Fig. \\ref{Fig_formalismo_dist_binomial}.\n",
    "\n",
    "Un cálculo un poco más largo permite ver que, ahora\n",
    "\\begin{align}\n",
    "\\overline X &=  np\\\\ \\rule{0mm}{7mm}\n",
    "\\sigma^2 &= n p(1-p)\n",
    "\\end{align}\n",
    "\n",
    "\\begin{figure}[H]\n",
    "\\centering \n",
    "\\includegraphics[width=0.4\\linewidth]{Figuras/Fig_formalismo_dist_binomial.png}\n",
    "\\caption{Distribución binomial}\n",
    "\\label{Fig_formalismo_dist_binomial}\n",
    "\\end{figure}\n",
    "\n",
    "\\SubsubiIt{La distribución Normal o Gaussiana}\n",
    "\n",
    "\\begin{mybox_gray2}{}\n",
    "La \\textbf{distribución normal} (o \\textbf{gaussiana}) centrada en $\\mu$ y con anchura $\\sigma$ viene dada por\n",
    "\\begin{equation}\n",
    "p(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp \\left({-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\right)\n",
    "\\end{equation}\n",
    "Nos encontramos ante una variable aleatoria con un espacio muestral continuo $x\\in (-\\infty,+\\infty)$. \n",
    "\\end{mybox_gray2}\n",
    "\n",
    "Podemos ver esta distribución en la Fig. \\ref{Fig_formalismo_dist_normal}. Tenemos además que\n",
    "\n",
    "\\begin{align}\n",
    "\\overline{X} &= \\int_{-\\infty}^{+\\infty} xp(x) dx = \\mu \\\\\n",
    "\\overline{(x-\\overline X)^2} &= \\int_{-\\infty}^{+\\infty} (x-\\mu)^2 p(x)dx =\\sigma^2\n",
    "\\end{align}\n",
    "\n",
    "\\begin{figure}[h] \\setcounter{subfigure}{0}\n",
    "\\centering\n",
    "\\subfigure[Diferentes valores de $\\mu$ y $\\sigma$]{\\includegraphics[width=0.4\\linewidth]{Figuras/Fig_formalismo_dist_normal.png}}\n",
    "\\hspace{0.5cm}\n",
    "\\subfigure[Distribución de probabilidad en torno a la media]{\\includegraphics[width=0.45\\linewidth]{Figuras/Fig_formalismo_dist_normal_2.png}}\n",
    "\\caption{Distribución normal (o gaussina)}\n",
    "\\label{Fig_formalismo_dist_normal}\n",
    "\\end{figure}\n",
    "\n",
    "\\subsection{Probabilidades combinadas}\n",
    "\n",
    "Las probabilidades combinadas son la base de las \\textbf{correlaciones}. Es aquí donde la Mecánica Cuántica produce resultados inesperados clásicamente. Esto es porque cantidades que son independientes desde el punto de vista clásico, presentan correlaciones al hacer el experimento. \n",
    "\n",
    "Ahora vamos a examinar variables aleatorias formadas por dos espacios muestrales $X$ e $Y$. Dependiendo de la forma en que combinemos la observación de cada una tendremos distintas distribuciones de probabilidad\n",
    "\n",
    "\\SubsubiIt{Probabilidad combinada}\n",
    "\n",
    "Una forma de lidiar con las correlaciones es tratar el conjunto de varias variables aleatorias como una sola variable. Esta es la idea detrás de la \\textbf{probabilidad combinada}, \n",
    "\n",
    "\\begin{mybox_gray2}{}\n",
    "La \\textbf{distribución de probabilidad combinada} asocia un número $p(x,y)$ a la probabilidad de observación conjunta de $x$ \\textbf{e} $y$. \n",
    "\n",
    "De esta forma, tratamos las parejas de eventos  como un solo evento  $a = (x,y)$. Por eso, la condición de normalización ahora es \n",
    "\\begin{equation}\n",
    "\\sum_a p(a) = \\sum_{xy} p(x,y) = 1\\, .\n",
    "\\end{equation}\n",
    "\\end{mybox_gray2}\n",
    "\n",
    "\\begin{itemize}\n",
    "\\item \\textbf{Distribución marginal}:\n",
    "\n",
    "La suma parcial sobre una de las dos variables conduce a sendas \\textbf{distribuciones marginales}\n",
    "\\begin{equation}\n",
    "q(x) = \\sum_{y} p(x,y) ~~~~~~~~~ \\tilde q(y) = \\sum_{x} p(x,y)\n",
    "\\end{equation}\n",
    "\n",
    "\\item \\textbf{Variables independientes}:\n",
    "\n",
    "Si dos variables aleatorias no presentan \\textbf{ninguna correlación}, decimos que son \\textbf{independientes}. En este caso las probabilidades combinadas factorizan \n",
    "\\begin{equation}\n",
    "p(x,y) = p(x) p(y)\n",
    "\\end{equation}\n",
    "La distribución de cada variable coincide con la que se deduce de marginalizar la otra\n",
    "\\begin{equation}\n",
    "\\sum_y p(x,y) = p(x)~~~~,~~~~\\sum_x p(x,y) = p(y)\n",
    "\\end{equation}\n",
    "\n",
    "\\end{itemize}\n",
    "\n",
    "\\SubsubiIt{Probabilidad condicionada}\n",
    "\n",
    "\\begin{mybox_gray2}{}\n",
    "La distribución de \\textbf{probabilidad condicionada} $p(X|Y)$ asigna un número $p(x|y)$ a la probabilidad  de encontrar un suceso $X=x$ una vez \\textit{sabemos} que $Y=y$ ha sido el resultado de consultar $Y$. \n",
    "\\end{mybox_gray2}\n",
    "\n",
    "La manera de acceder experimentalmente a estas distribuciones, es efectuar un muestreo $(a_i,b_i), i=1,...,N$ de valores de $(X,Y)$ y \\textit{seleccionar} sólo aquellos sucesos donde $b_i = y$ un valor concreto de $Y$.\n",
    "\n",
    "\\Teorema{ (\\textbf{Teorema de Bayes}): Las probabilidades condicionales y combinadas se relacionan de la forma siguiente\n",
    "\\begin{equation}\n",
    "p(x,y) = p(x|y)p(y) = p(y|x) p(x)\n",
    "\\end{equation}\n",
    "La segunda igualdad conduce al \\textbf{teorema de Bayes}\n",
    "\\begin{equation}\n",
    "p(x|y) = \\frac{p(y|x)p(x)}{p(y)}\n",
    "\\end{equation}\n",
    "}\n",
    "\n",
    "\\subsection{Entropía de una una variable aleatoria}\n",
    "\n",
    "\\Definicion{\n",
    "Dada una variable aleatoria $(X,p(X))$ definimos la \\textbf{entropía} asociada mediante la expresión\n",
    "\\begin{equation}\n",
    "H = -\\sum_x p(x)\\log p(x)\n",
    "\\end{equation}\n",
    "}\n",
    "\n",
    "\\begin{itemize}\n",
    "\\item El signo negativo hace esta expresión positiva, debido a que $\\log p(x)\\leq 0$.\n",
    "\n",
    "\\item El valor de $H$ está acotado entre $H\\in [0,\\log(N)]$ donde $N$ es el número de sucesos $x$ posibles\n",
    "\n",
    "\\item Si $p(x_i)=0 \\,\\,  \\forall x_i$ excepto $p(x_0)=1$ para un evento posible $\\Rightarrow H=0$\n",
    "\n",
    "\\item Si $p(x_i) = 1/N$ es equiprobable $\\Rightarrow H = \\log(N)$ y este es el valor máximo. \n",
    "\n",
    "\\end{itemize}\n",
    "\n",
    "\\begin{mybox_green}{Ejemplo}\n",
    "La entropía de la distribución de Bernoulli es \n",
    "\\begin{equation}\n",
    "H = - p\\log p -(1-p)\\log (1-p)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{figure}[H]\n",
    "\\centering \n",
    "\\includegraphics[width=0.5\\linewidth]{Figuras/Fig_formalismo_binaryentropy.png}\n",
    "\\caption{Entropía para una distribución de Bernouilli}\n",
    "\\label{Fig_formalismo_binaryentropy}\n",
    "\\end{figure}\n",
    "\\end{mybox_green}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8534e04c",
   "metadata": {},
   "source": [
    "<figure><center>\n",
    "<img   src=\"https://quantumspain-project.es/wp-content/uploads/2022/11/Logo_QS_EspanaDigital.png\" align=center  width=\"2000px\"/>\n",
    "</center></figure>\n",
    "\n",
    "<center>\n",
    "<img align=\"left\" src=\"https://quantumspain-project.es/wp-content/uploads/2024/02/Banner-QS_GOB_v2.png\" width=\"1000px\" />\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
