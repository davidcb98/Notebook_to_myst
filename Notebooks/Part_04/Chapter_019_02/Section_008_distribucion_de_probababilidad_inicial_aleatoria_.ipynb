{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ddaeb38",
   "metadata": {},
   "source": [
    "<figure><center>\n",
    "<img   src=\"Figuras/Fig_logo_UMA_scbi.png\" align=center  width=\"2000px\"/>\n",
    "</center></figure>\n",
    "\n",
    "$ \\newcommand{\\bra}[1]{\\langle #1|} $\n",
    "$ \\newcommand{\\ket}[1]{|#1\\rangle} $\n",
    "$ \\newcommand{\\branew}[1]{\\langle #1|} $\n",
    "$ \\newcommand{\\ketnew}[1]{\\langle #1|} $\n",
    "$ \\newcommand{\\braket}[2]{\\langle #1|#2\\rangle} $\n",
    "$ \\newcommand{\\ketbra}[2]{| #1\\rangle \\langle #2 |} $\n",
    "$ \\newcommand{\\i}{{\\color{blue} i}} $ \n",
    "$ \\newcommand{\\Hil}{{\\cal H}} $\n",
    "$ \\newcommand{\\cg}[1]{{\\rm C}#1} $\n",
    "$ \\newcommand{\\lp}{\\left(} $\n",
    "$ \\newcommand{\\rp}{\\right)} $\n",
    "$ \\newcommand{\\lc}{\\left[} $\n",
    "$ \\newcommand{\\rc}{\\right]} $\n",
    "$ \\newcommand{\\lch}{\\left\\{} $\n",
    "$ \\newcommand{\\rch}{\\right\\}} $\n",
    "$ \\newcommand{\\Lp}{\\Bigl(} $\n",
    "$ \\newcommand{\\Rp}{\\Bigr)} $\n",
    "$ \\newcommand{\\Lc}{\\Bigl[} $\n",
    "$ \\newcommand{\\Rc}{\\Bigr]} $\n",
    "$ \\newcommand{\\Lch}{\\Bigl\\{} $\n",
    "$ \\newcommand{\\Rch}{\\Bigr\\}} $\n",
    "$ \\newcommand{\\rqa}{\\quad \\Rightarrow \\quad} $\n",
    "$ \\newcommand{\\bm}{\\boldsymbol}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "\n",
    "<a id='sec_arbitrary_distribition'></a>\n",
    "# Distribución de probababilidad inicial aleatoria \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "En esta sección vamos a ver una generalización del algoritmo de Grover para el caso en el que partimos de una distribución de probabilidad aleatoria (no uniforme). Veremos como este algoritmo generalizado sigue requiriendo un número de iteraciones $\\mathcal{O}(\\sqrt{N/M})$, aunque veremos también que hay ciertos casos particularmente desfavorables donde no podemos encontrar ninguna solución.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "Este caso es de especial interés si tenemos en cuenta que muchas veces hay errores al aplicar las puertas. De esta forma, podemos tener errores en el paso de inicialización de la distribución uniforme y acabar con una distribución que se distancia un poco de esta. Como veremos a continuación, el algoritmo de Grover sigue funcionando en presencia de estos errores modestos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "En la siguientes subsecciones presentaremos el algoritmo y derivaremos las ecuaciones diferenciales que rigen la evolución de las amplitudes. Usaremos la solución (exacta) de las mismas para calcular la probabilidad de éxito y analizaremos la diferente casuística. Como el cálculo, aunque simple, puede hacerse pesado, se incluye al final un resumen de con las conclusiones importantes (sección \\ref{sec_prop-no-uni_resumen}).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "## Algoritmo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "En realidad, el algoritmo no tiene ningún misterio. Consiste simplemente en saltarse al paso de la inicialización con las puertas de Hadammard y partir del estado con una distribución aleatoria:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "\n",
    " \n1. Utilizar cualquier distribución inicial, por ejemplo, el estado final de cualquier otro algoritmo cuántico (no inicializar el sistema con la distribución uniforme).\n",
    " \n2. Aplicar el operador de Grover $T$ veces (calcularemos $T$). El operador de Grover al que nos referimos aquí es el mismo de las anteriores secciones (con la Walsh-Hadammard o con una trasformación de la forma (\\ref{ec_N-no-2n_T})). No cambia nada en ese sentido.\n",
    " \n3. Medir el resultado\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "\n",
    "<a id='subsec_prop-no-uni_ecuaciones-diff'></a>\n",
    "## Evolución de las amplitudes ($M$ soluciones) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "Analicemos la evolución de las amplitudes en el algoritmo modificado. Supongamos (como siempre) que tenemos $N$ estados y $M$ soluciones. Asumamos, sin perdida de generalidad, que $0 \\leq M \\leq N/2$. El estado total ahora será de la forma\n",
    "\\begin{equation} \\label{ec_prob-no-uni_phi-j}\n",
    "\\boxed{|\\Psi(t) \\rangle = \\sum_{i \\in \\omega} k_i(t) |i \\rangle + \\sum_{i \\in \\omega^\\perp} l_i(t) |i \\rangle }.\n",
    "\\end{equation}\n",
    "Nótese que ahora las amplitudes tenemos un índices $i$ que corresponde al estado al que acompañan. Esto es fácil de entender: como la distribución de partida ahora no es uniforme, cada estado tendrá su amplitud. Las medias de las amplitudes de los estados solución ($i \\in \\omega$) y no solución ($i \\in \\omega^\\perp$) son\n",
    "\\begin{align} \\label{ec_prop-no-uni_k-l-mean}\n",
    "& \\boxed{\\bar{k}(t) = \\frac{1}{M} \\sum_{i \\in \\omega} k_i(t) }\n",
    "& \\boxed{\\bar{l}(t) = \\frac{1}{N-M} \\sum_{i \\in \\omega^\\perp} l_i(t)}.\n",
    "\\end{align}\n",
    "Una de las conclusiones clave del desarrollo que vamos a ver a continuación es que <i>la dinámica dictada por el algoritmo de Grover se puede describir por completo basándose en la dependencia de las medias de las amplitudes con las iteraciones</i>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "### Derivación de las ecuaciones diferenciales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "Comencemos viendo la expresión de la media total de las amplitudes el estado $|\\Psi(t) \\rangle$\n",
    "\\begin{equation}\n",
    "\\bar{\\psi}(t) = \\frac{1}{N} \\lc M \\bar{k}(t) + (N-M) \\bar{l}(t) \\rc,\n",
    "\\end{equation}\n",
    "y definamos la cantidad $C(t)$\n",
    "\\begin{equation}\n",
    "C(t) = \\frac{2}{N} \\lc (N-M) \\bar{l}(t) - M \\bar{k}(t) \\rc.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "Veamos como evolucionan las medias de las amplitudes con cada iteracións:\n",
    "\n",
    " \n- Después a aplicar el oráculo sobre el esto $|\\Psi(t) \\rangle$ lo que sucede es el signo de la amplitud de los estados solución se invierte, con lo que\n",
    "    {\n",
    "    \\extrarowheight = -0.5ex\n",
    "    \\renewcommand{\\arraystretch}{1.75}\n",
    "    \\begin{equation*}\n",
    "    \\lch \\begin{matrix}\n",
    "    & \\bar{k}(t) \\, \\rightarrow \\, \\bar{k^{'}}(t) = -\\bar{k}(t) \\\\\n",
    "    & \\bar{l}(t) \\, \\rightarrow \\, \\bar{l^{'}}(t) = \\bar{l}(t)\n",
    "    \\end{matrix} \\right.\n",
    "    \\rqa\n",
    "    \\overline{U_\\omega \\psi}(t) = \\frac{1}{N} \\lc - M\\bar{k}(t) + (N-M) \\bar{l}(t) \\rc = \\frac{C(t)}{2}.\n",
    "    \\end{equation*}\n",
    "    }\n",
    " \n- Después a aplicar el difusor sobre el esto $|\\Psi(t) \\rangle$ lo que sucede es una reflexión respecto a la media $\\overline{U_\\omega \\psi}(t)$. Esto se pude expresar de la siguiente manera\n",
    "    \\begin{align}\n",
    "    {k}^{'}_i(t) \\, & \\rightarrow \\, {k}^{\"}_i(t) =  2 \\overline{U_\\omega \\psi}(t) - {k}^{'}_i(t) =  C(t) - k^{'}_i(t) = \\boxed{ C(t) + k_i(t) = k_i(t+1)}  \\label{ec_prop-no-uni_k-j+1_i} \\\\\n",
    "    {l}^{'}_i(t) \\, & \\rightarrow \\, {l}^{\"}_i(t) =  2 \\overline{U_\\omega \\psi}(t) - {l}^{'}_i(t) = C(t) - l^{'}_i(t) = \\boxed{C(t) - l_i(t) = l_i(t+1)}. \\label{ec_prop-no-uni_l-j+1_i}\n",
    "    \\end{align}\n",
    "    Promediando\n",
    "    \\begin{align}\n",
    "    \\Aboxed{\\bar{k}(t+1) = C_j + \\bar{k}(t) } \\label{ec_prop-no-uni_k-j+1} \\\\\n",
    "    \\Aboxed{\\bar{l}(t+1) = C_j - \\bar{l}(t)}.  \\label{ec_prop-no-uni_l-j+1}\n",
    "    \\end{align}\n",
    "\n",
    "Las Ecs. (\\ref{ec_prop-no-uni_k-j+1}) y (\\ref{ec_prop-no-uni_l-j+1}) son las ecuaciones diferenciales que rigen la evolución de la media de las amplitudes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "### Soluciones de las ecuaciones diferenciales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "Vamos a resolver de forma analítica las Ecs. (\\ref{ec_prop-no-uni_k-j+1}) y (\\ref{ec_prop-no-uni_l-j+1})  para ver la evolución exacta de las medias de las amplitudes de los estados solución y no solución. Procedemos a resolver las fórmulas de recursión para condiciones iniciales complejas arbitrarias. Empecemos definiendo las funciones\n",
    "\\begin{align}\n",
    "\\Aboxed{& f_{+}(t) = \\bar{l}(t) + i \\sqrt{\\frac{M}{N-M}} \\bar{k}(t)} \\label{ec_prop-no-uni_f+}\\\\\n",
    "\\Aboxed{& f_{-}(t) = \\bar{l}(t) - i \\sqrt{\\frac{M}{N-M}} \\bar{k}(t)}. \\label{ec_prop-no-uni_f-}\n",
    "\\end{align}\n",
    "Vemos que no son más que un <b>cambio de variables</b> para facilitarnos la resolución de las ecuaciones. Empleando este cambio de variables podemos reescribir las Ecs. (\\ref{ec_prop-no-uni_k-j+1}) y (\\ref{ec_prop-no-uni_l-j+1}) de la siguiente forma\n",
    "\\begin{align}\n",
    "& f_{+}(t+1) = e^{i \\beta} f_{+}(t) \\\\\n",
    "& f_{-}(t+1) = e^{- i \\beta} f_{-}(t),\n",
    "\\end{align}\n",
    "donde $\\beta$ es <b>real</b> y cumple\n",
    "\\begin{equation} \\label{ec_prop-no-ini_beta}\n",
    "\\boxed{\\cos \\beta = 1 - 2 \\frac{M}{N}}.\n",
    "\\end{equation}\n",
    "Vemos que ahora la solución de las ecuaciones es trivial:\n",
    "\\begin{align}\n",
    "& f_{+}(t) = e^{i \\beta t} f_{+}(0) \\\\\n",
    "& f_{-}(t) = e^{-i \\beta t} f_{-}(0).\n",
    "\\end{align}\n",
    "Vemos claramente que $| f_{+}(0) |$ y $|f_{-}(0)|$ son independientes de la iteración $t$ en la que estemos. Podemos ahora deshacer el cambio de variable\n",
    "\\begin{align}\n",
    "\\Aboxed{\\bar{k}(t) & = - i \\sqrt{\\frac{N-M}{4M}} \\lc e^{i \\beta t} f_{+}(0) - e^{-i \\beta t} f_{-}(0)  \\rc}  \\label{ec_prop-no-uni_sol_k} \\\\\n",
    "\\Aboxed{\\bar{l}(t) & = \\frac{1}{2} \\lc e^{i \\beta t} f_{+}(0) - e^{-i \\beta t} f_{-}(0)  \\rc }.  \\label{ec_prop-no-uni_sol_l}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "## Propiedades de las soluciones: Probabilidad de acierto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "### Evolución de las amplitudes en función de la evolución de las medias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "Lo primero que vamos a hacer es ver que, efectivamente, solo con la evolución de las medias podemos describir la evolución.\n",
    "Para ello, restemos a la Ec. (\\ref{ec_prop-no-uni_k-j+1_i}) la Ec. (\\ref{ec_prop-no-uni_k-j+1}) y a la Ec. (\\ref{ec_prop-no-uni_l-j+1_i}) restémosle la Ec. (\\ref{ec_prop-no-uni_l-j+1})\n",
    "\\begin{align*}\n",
    "k_i(t+1) - \\bar{k}(t+1) & = k_i(t) - \\bar{k}(t) \\\\\n",
    "l_i(t+1) - \\bar{l}(t+1) & = - \\lc l_i(t) - \\bar{l}(t) \\rc.\n",
    "\\end{align*}\n",
    "Esto significa que las cantidades\n",
    "\\begin{align}\n",
    "\\Delta k_i & \\equiv k_i(0) - \\bar{k}(0) \\\\\n",
    "\\Delta l_i & \\equiv l_i(0) - \\bar{l}(0)\n",
    "\\end{align}\n",
    "son <i>constantes del movimiento</i>. Esto nos permite simplificar las expresiones para la dependencia temporal de las amplitudes:\n",
    "\\begin{align}\n",
    "\\Aboxed{k_i(t) & = \\bar{k}(t) + \\Delta k_i} \\label{ec_prop-no-uni_k-delta}\\\\\n",
    "\\Aboxed{l_i(t) & = \\bar{l}(t) + (-1)^t \\Delta l_i}. \\label{ec_prop-no-uni_l-delta}\n",
    "\\end{align}\n",
    "Vemos que  la distribución de las amplitudes de los estados solución $k_i(t)$ respecto a su medias $\\bar{k}(t)$ es constante. Es decir, las amplitudes varían al unísono entorno a la medias. De esta forma, con saber las distribución inicial respecto a las medía ($\\Delta k_i$) podemos describir la evolución de las amplitudes solo conociendo la evolución de las medias. Lo mismo sucede para los estados no solución $l_i(t)$, salvo que estos se invierten entorno a su media en cada iteración.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "De las Ecs. (\\ref{ec_prop-no-uni_k-delta}) y (\\ref{ec_prop-no-uni_l-delta}) se ve inmediatamente que las varianzas\n",
    "\\begin{align*}\n",
    "\\sigma_k^2 (t) & = \\frac{1}{M} \\sum_{i \\in \\omega} |k_i(t) - \\bar{k}(t)|^2   \\\\\n",
    "\\sigma_l^2 (t) & = \\frac{1}{N-M} \\sum_{i \\in \\omega^\\perp} |l_i(t) - \\bar{l}(t)|^2\n",
    "\\end{align*}\n",
    "son independientes del tiempo [$\\sigma_k^2 (t) = \\sigma_k^2 (0)$ y $\\sigma_l^2 (t)=\\sigma_l^2 (0)$ $ \\forall t$].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "\n",
    "<a id='subsec_prop-no-uni_Pt'></a>\n",
    "### Probabilidad de acierto. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "Para facilitar el análisis de las soluciones, definamos las variables (complejas) $\\alpha$ y $\\phi$ de la forma\n",
    "\\begin{align} \\label{ec_prop-no-uni_alpha_phi}\n",
    "& \\alpha = \\sqrt{f_{+}(0) f_{-}(0)},\n",
    "& e^{2i\\phi} = f_{+}(0)/f_{-}(0).\n",
    "\\end{align}\n",
    "Reescribamos las Ecs. (\\ref{ec_prop-no-uni_sol_k}) y (\\ref{ec_prop-no-uni_sol_l}) en función de estas variables\n",
    "\\begin{align}\n",
    "\\Aboxed{\\bar{k}(t) & = \\sqrt{\\frac{N-M}{M}} \\alpha \\sin ( \\beta t + \\phi)}  \\label{ec_prop-no-uni_sol_k-2} \\\\\n",
    "\\Aboxed{\\bar{l}(t) & = \\alpha \\cos (\\beta t + \\phi)} \\label{ec_prop-no-uni_sol_l-2}\n",
    "\\end{align}\n",
    "Esto nos permite ver que hay una diferencia de fase de $\\pi/2$ entre las medias de las amplitudes de los estados solución y los no solución. Se ve fácilmente que cuando una es máxima, la otra es mínima.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "La probabilidad de medir alguno de los estados solución será $P(t) = \\sum_{i \\in \\omega} |k_i(t)|^2$. Como todos los operadores son unitarios, las amplitudes cumplen la condición de normalización:\n",
    "\\begin{equation} \\label{ec_prop-no-uni_unitaridad}\n",
    "\\sum_{i \\in \\omega} |k_i(t)|^2 + \\sum_{i \\in \\omega^\\perp} |l_i(t)|^2 = 1 \\quad \\forall t\n",
    "\\end{equation}\n",
    "Usando la identidad\n",
    "\\begin{equation}\n",
    "\\overline{(y-\\bar{y})^2} = \\overline{y^2} - \\bar{y}^2 \\rqa \\sigma_y^2 = \\overline{y^2} - \\bar{y}^2\n",
    "\\rqa  \\overline{y^2} = \\sigma_y^2  + \\bar{y}^2\n",
    "\\end{equation}\n",
    "podemos escribir\n",
    "\\begin{align*}\n",
    "\\overline{|k_i(t)|^2} = \\frac{1}{M} \\sum_{i \\in \\omega} |k_i (t) |^2 = \\sigma_k^2 + |\\bar{k}(t)|^2 \\\\\n",
    "\\overline{|l_i(t)|^2} = \\frac{1}{N-M} \\sum_{i \\in \\omega} |l_i (t) |^2 = \\sigma_l^2 + |\\bar{l}(t)|^2\n",
    "\\end{align*}\n",
    "Despejando $\\sum_{i \\in \\omega} |k_i(t)|^2$ en (\\ref{ec_prop-no-uni_unitaridad}), sustituyendo en $P(t)$, usando la expresión anterior de $\\sum_{i \\in \\omega} |l_i (t) |^2 $ y desarrollando con cuidado, puede verse que \n",
    "\\begin{equation} \\label{ec_prop-no-uni_Pt}\n",
    "\\boxed{P(t) = P_{av} - \\Delta P \\cos 2\\lc \\beta t + \\text{Re}(\\phi) \\rc ,}\n",
    "\\end{equation}\n",
    "donde\n",
    "\\begin{align*}\n",
    "& P_{av} = 1- (N-M) \\sigma_l^2 - \\frac{1}{2} \\Lc (N-r) |\\bar{l}(0)|^2 + r |\\bar{k}(0)|^2 \\Rc \\\\\n",
    "& \\Delta P = \\frac{1}{2} \\Bigl| (N-M) \\bar{l}(0)^2 + M \\bar{k}(0)^2 \\Bigr|\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "\n",
    "<a id='subsec_prop-no-uni_T'></a>\n",
    "### Número optimo, $T$, de iteraciones. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "El valor máximo de la probabilidad que se puede obtener durante la evolución del algoritmo es\n",
    "\\begin{equation}\n",
    "P_{max}=P_{av} + \\Delta P\n",
    "\\end{equation}\n",
    "Dada una distribución arbitraria inicial de $M$ estados solución y $N-M$ estados no solución, con medias $\\bar{k}(0)$ y $\\bar{l}(0)$ respectivamente, el valor máximo de la probabilidad $P_{max}$ se alcanzará cuando realicemos $T$ iteraciones tal que\n",
    "\\begin{equation} \\label{ec_prop-no-uni_T}\n",
    "\\cos 2 \\lc \\beta T + \\text{Re} (\\phi) \\rc = -1 \\rqa\n",
    "\\boxed{T = \\lc (u + 1/2)  \\pi - \\text{Re} (\\phi) \\rc/\\beta}\n",
    "\\end{equation}\n",
    "con $u = 0,1,2,\\dots$. Una importante conclusión es que para determinar el valor óptimo de iteraciones, todo lo que necesitamos es conocer <i>las medias iniciales de las amplitudes y el número de estados marcados</i>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "Si $M$ es pequeño tenemos que $1-2M/N \\approx 1$, así que podemos aproximar $\\beta$ a segundo orden en la Ec. (\\ref{ec_prop-no-ini_beta})\n",
    "\\begin{equation}\n",
    "\\cos \\beta \\approx 1 - \\frac{1}{2} \\beta^2 = 1- 2 \\frac{M}{N} \\rqa \\beta = 2 \\sqrt{\\frac{M}{N}}\n",
    "\\end{equation}\n",
    "Así que tenemos que $T$ es del orden de $\\mathcal{O}(\\sqrt{N/M})$ (para $u=0$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "\n",
    "<a id='subsec_prop-no-uni_casos-particulares'></a>\n",
    "### Casos particulares. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "El valor máximo de la probabilidad puede variar mucho dependiendo de las propiedades estáticas (medias y varianzas) de las distribución inicial de amplitudes. Como es lógico, cuanto mayor sea $P_{max}$ menos repeticiones del algoritmo tendremos que hacer para encontrar una solución (donde cada repetición son $m$ iteraciones). Es fácil darse cuenta de que el número esperado de repeticiones del algoritmo hasta encontrar un estado solución es $1/P_{max}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<p style=\"color: DarkRed;\">\n",
    "<b>Nota</b>:\n",
    "<br>\n",
    "Porque <i>repeticiones del algoritmo</i>? En el parágrafo anterior llamamos repeticiones del algoritmo a realizar varias veces las $m$ iteraciones necesarias para maximizar la probabilidad.\n",
    "<br>\n",
    "Si la probabilidad máxima es menor de uno, tenemos entonces la posibilidad de tras realizar las $m$ iteraciones no midamos un estado solución. Si esto sucede, como al medir se destruye el estado, lo que hay que hacer es volver a empezar: volver a partir de la distribución inicial y aplicar $m$ iteraciones. En promedio, tendremos que repetir este proceso $1/P_{max}$ veces. Vemos que si la probabilidad máxima es, por ejemplo, $1/4$, tendremos que medir en promedio 4 veces para obtener un estado solución.\n",
    "</p></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "Veamos los diferentes casos:\n",
    "\n",
    " \n- Cuando el ratio $\\bar{l}(0)/\\bar{k}(0)$ es real, puede verse fácilmente que $|f_+(0)| =|f_{-}(0)|$. En este caso $P_{max} = 1-(N-r)\\sigma_l^2$. El mejor caso, aquel con $P_{max}=1$, se obtiene cuando $\\sigma_l=0$, es decir, cuando la distribución inicial es uniforme.\n",
    " \n- Cuando tenemos $f_+(0)=0$ o $f_-(0)=0$, el algoritmo es inútil, pues $P(t)$ es constante.\n",
    " \n- El peor caso se da cuando $\\sigma_k^2=f_+(0)=f_-(0)=\\bar{k} (0)=\\bar{l}(0)=0$ y $(N-r)\\sigma_l^2=1$. En este caso tenemos $P_{max}=P(t)=0$ $\\forall t$, con lo que nunca encontraremos una solución.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "\n",
    "<a id='subsec_prop-no-uni_desconocida'></a>\n",
    "### Distribución de probabilidad desconocida. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "Veamos que sucede si no conocemos de antemano las medias y las varianzas de la distribución inicial de amplitudes.\n",
    "La solución es más sencilla de lo que podríamos pensar en un principio. Lo único que hay que hacer es ejecutar el algoritmo dos veces, ejecutando en un caso $m_1$ iteraciones y en el otro $m_2$ iteraciones, tal que $m_2-m_1=\\pi/(2 \\beta)$. De la Ec. (\\ref{ec_prop-no-uni_Pt}) está claro que al menos en uno de los dos caso vamos a tener $P(t)\\geq P_{av} \\geq P_{max}/2$. En este caso, necesitamos el doble de repeticiones para obtener al menos la mitad de probabilidades de éxito que cuando se conoce el tiempo de medición óptimo. Vemos además que lo único que necesitamos conocer en este caso es $\\beta$, que según la Ec. (\\ref{ec_prop-no-ini_beta}) podemos calcularlo a partir del número de soluciones $M$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "\n",
    "<a id='sec_prop-no-uni_resumen'></a>\n",
    "## Resumen de la sección.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "Uno de los puntos importantes del análisis de esta sección es el hecho de demostrar que el algoritmo de Grover funciona incluso en el caso en el que tenemos pequeños errores a la hora de generar la distribución de probabilidad inicial. En realidad, en esta sección no se presenta ningún algoritmo nuevo. Simplemente se analiza que pasa si usamos el algoritmo de Grover donde en el primer paso sustituimos la inicialización de la distribución uniforme por una distribución no uniforme.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "Otra de las conclusiones clave del desarrollo es que la dinámica dictada por el algoritmo de Grover se puede describir por completo basándose en la dependencia de las medias de las amplitudes $\\bar{k}(t)$ y $\\bar{l}(t)$ (definidas en la Ec. (\\ref{ec_prop-no-uni_k-l-mean})).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "Para estudiar la dependencia con las iteraciones de las medias $\\bar{k}(t)$ y $\\bar{l}(t)$, en la sección \\ref{subsec_prop-no-uni_ecuaciones-diff} deducimos las ecuaciones diferenciales que rigen su evolución (Ecs. (\\ref{ec_prop-no-uni_k-j+1}) y (\\ref{ec_prop-no-uni_l-j+1})) y las resolvemos. Para ello primero hacemos un cambio de variable de $\\bar{k}(t)$ y $\\bar{l}(t)$ a $f_+(t)$ y $f_-(t)$ (Ecs. (\\ref{ec_prop-no-uni_f+}) y (\\ref{ec_prop-no-uni_f-})).  Resolvemos usando estas variables y deshacemos el cambio. Nos quedan pues $\\bar{k}(t)$ y $\\bar{l}(t)$ en función de $f_+(0)$, $f_-(0)$ y $\\beta$ (definido $\\beta$ en la Ec. (\\ref{ec_prop-no-ini_beta}), que solo depende de $N$ y $M$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "Una vez tenemos las soluciones, en las Ecs. (\\ref{ec_prop-no-uni_k-delta}) y (\\ref{ec_prop-no-uni_l-delta}) demostramos que las amplitudes mantienen su distribución respecto a las medias invariante (con una salvedad en $\\bar{l}(t)$). De esta forma, confirmamos que solo tenemos que conocer la distribución inicial respecto a las medias para describir la evolución de las amplitudes usando solo la evolución de las medias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "En la sección \\ref{subsec_prop-no-uni_Pt}  definimos dos parámetros más, $\\alpha$ y $\\phi$ (Ec. (\\ref{ec_prop-no-uni_alpha_phi})), que depende de $f_+(0)$ y $f_-(0)$. Tras unas líneas de cálculo podemos llegar a la Ec. (\\ref{ec_prop-no-uni_Pt}) que nos da la probabilidad $P(t)$ de medir un estado solución en función del número de iteraciones. Una vez tenemos esta expresión, en la sección \\ref{subsec_prop-no-uni_T} buscamos el número $T$ de iteraciones que nos maximizan la probabilidad (Ec. (\\ref{ec_prop-no-uni_T})). Una importante conclusión es que para determinar el valor óptimo de iteraciones, todo lo que necesitamos es conocer las medias iniciales de las amplitudes y el número de estados marcados. Concluimos también que si $M$ es pequeño, tenemos que $T$ es del orden de $\\mathcal{O}(\\sqrt{N/M})$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe82e",
   "metadata": {},
   "source": [
    "Finalmente, analizamos casos particulares de distribuciones iniciales favorables y desfavorables (sección \\ref{subsec_prop-no-uni_casos-particulares}) y vemos que pasa si no conocemos las distribución de probabilidad inicial. Es este último caso, nuevamente podemos hallar una solución en tiempo $\\mathcal{O}(N/M)$ conociendo solo $M$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8534e04c",
   "metadata": {},
   "source": [
    "<figure><center>\n",
    "<img   src=\"https://quantumspain-project.es/wp-content/uploads/2022/11/Logo_QS_EspanaDigital.png\" align=center  width=\"2000px\"/>\n",
    "</center></figure>\n",
    "\n",
    "<center>\n",
    "<img align=\"left\" src=\"https://quantumspain-project.es/wp-content/uploads/2024/02/Banner-QS_GOB_v2.png\" width=\"1000px\" />\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
